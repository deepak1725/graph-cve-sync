#!/usr/bin/env python3
# Copyright Â© 2020 Red Hat Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Author: Yusuf Zainee <yzainee@redhat.com>
#

"""Script which synchronizes snyk CVEs from S3 to graph."""

from datetime import datetime, timedelta
import boto3
import json
import os
from f8a_utils.versions import get_versions_and_latest_for_ep
from unified_range import api
from f8a_version_comparator.comparable_version import ComparableVersion
import re
import logging
import requests
import time


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Helper:
    """Helper class with some utility functions."""

    def __init__(self):
        """Init method for helper class."""
        self.CVE_BUCKET = os.environ.get("REPORT_BUCKET_NAME", '')
        self.AWS_KEY = os.environ.get("AWS_S3_ACCESS_KEY_ID_REPORT_BUCKET", '')
        self.AWS_SECRET = os.environ.get("AWS_S3_SECRET_ACCESS_KEY_REPORT_BUCKET", '')
        self.AWS_REGION = os.environ.get("AWS_S3_REGION", "us-east-1")
        self.HOST = os.environ.get('BAYESIAN_DATA_IMPORTER_SERVICE_HOST', 'bayesian-data-importer')
        self.PORT = os.environ.get('BAYESIAN_DATA_IMPORTER_SERVICE_PORT', '9192')

        self.s3_resource = boto3.resource('s3', aws_access_key_id=self.AWS_KEY,
                                          aws_secret_access_key=self.AWS_SECRET,
                                          region_name=self.AWS_REGION)

    def store_json_content(self, content, obj_key):
        """Store the report content to the S3 storage."""
        try:
            logger.info('Storing the data into the S3 file %s' % obj_key)
            self.s3_resource.Object(self.CVE_BUCKET, obj_key).put(
                Body=json.dumps(content, indent=2).encode('utf-8'))
        except Exception as e:
            logger.exception('%r' % e)

    def _retrieve_dict(self, object_key):
        """Retrieve a dictionary stored as JSON from S3."""
        return json.loads(self._retrieve_blob(object_key).decode('utf-8'))

    def _retrieve_blob(self, object_key):
        """Retrieve remote object content."""
        return self.s3_resource.Object(self.CVE_BUCKET, object_key).get()['Body'].read()

    def read_data_from_s3(self, date, loc):
        """Read the snyk data from S3."""
        try:
            filename = loc + date + ".json"
            logger.info('Retrieving the data from the S3 file %s' % filename)
            return self._retrieve_dict(filename)
        except Exception as e:
            logger.error(e)
            return False

    def make_api_call(self, payload, mode):
        """Make an API call to data importer."""
        try:
            api_url = "http://" + self.HOST + ":" + self.PORT
            headers = {'Content-type': 'application/json'}
            msg = ""
            response = ""
            if mode == "PUT":
                response = requests.put('{}/api/v1/snyk-cves'.format(api_url), headers=headers,
                                        data=json.dumps(payload))
                msg = "ingested"
            elif mode == "DELETE":
                response = requests.delete('{}/api/v1/snyk-cves'.format(api_url), headers=headers,
                                           data=json.dumps(payload))
                msg = "deleted"
            if response.status_code == 200:
                logger.info("CVEs for {p} successfully {m}".format(p=payload['package'], m=msg))
            elif response.status_code == 504:
                logger.info("Operation for {p} is taking time. "
                            "Will be {m} in the background".format(p=payload['package'], m=msg))
            else:
                logger.info("Error while {m} CVEs for {p}".format(p=payload['package'], m=msg))
                logger.info("Status Code {}".format(response.status_code))
        except Exception:
            logger.error("API call to data importer failed.")

    def is_dry_run(self):
        """Return True if this is a dry run."""
        return os.environ.get('SNYK_DRY_RUN', 'false').lower() in ('1', 'yes', 'true')

    def force_run_ingestion(self):
        """Return if ingestion mode is on."""
        return os.environ.get('SNYK_INGESTION_FORCE_RUN', 'false').lower() in ('1', 'yes', 'true')

    def is_delta_mode_on(self):
        """Return if the delta feed mode is on."""
        return os.environ.get('SNYK_DELTA_FEED_MODE', 'false').lower() in ('1', 'yes', 'true')


class SnykCveSync:
    """Snyk class to sync cves to graph."""

    def __init__(self, data):
        """Init method for helper class."""
        self.helper = Helper()
        self.CVE_DATA = {}
        self.DELETE_CVE_DATA = {}
        self.DELTA_FEED = {}
        self.utc_now = datetime.utcnow()
        self.day = self.utc_now.weekday()
        self.yesterday = (self.utc_now - timedelta(days=1)).strftime('%Y-%m-%d')
        self.selective_eco_run = os.environ.get('SELECTIVE_ECOSYSTEM_SNYK_SYNC', 'false')
        if not data:
            self.snyk_data = self.helper.read_data_from_s3(self.utc_now.strftime('%d-%m-%y'),
                                                           "snyk-feed/")
        else:
            self.snyk_data = data

    def _parse_data_for_eco(self, eco):
        """Return True/False depending on whether the ecosysytem data should be parsed or not."""
        if self.helper.is_delta_mode_on():
            return True
        if eco == self.selective_eco_run:
            return True
        elif self.selective_eco_run != "false":
            return False
        day = str(self.day)
        if (day in ["0", "3"] and eco == "java") \
                or (day in ["1", "4"] and eco == "js") \
                or (day in ["2", "5"] and eco == "python"):
            return True
        return False

    def _get_version_rules(self, vuln_versions):
        """Version rules for pypi,maven eco."""
        rules = []
        regex_op = "[0-9a-zA-Z\\_\\.\\-]+"
        regex_vr = "[<>=*]+"
        for version in vuln_versions:
            version = version.replace(" ", "")
            sub_vers = version.split('||')
            for sub_ver in sub_vers:
                tmp = []
                vr_relations = re.split(regex_vr, sub_ver)
                op_relations = re.split(regex_op, sub_ver)
                if len(vr_relations) == 1:
                    tmp.append({
                        'key': "=",
                        'val': ComparableVersion(vr_relations[0])
                    })
                elif len(op_relations) == 1 and op_relations[0] == '*':
                    tmp.append({
                        'key': "*",
                        'val': ""
                    })
                else:
                    for i in range(len(op_relations) - 1):
                        tmp.append({
                            'key': op_relations[i],
                            'val': ComparableVersion(vr_relations[i + 1])
                        })
                rules.append(tmp)

        return rules

    def _is_relation_applicable(self, key, version, rule):
        """Check if the version satisfies the relation."""
        if key == '<':
            return ComparableVersion(version) < rule
        elif key == '>':
            return ComparableVersion(version) > rule
        elif key == '=':
            return ComparableVersion(version) == rule
        elif key == '<=':
            return ComparableVersion(version) <= rule
        elif key == '>=':
            return ComparableVersion(version) >= rule
        elif key == '*':
            return True

    def _get_affected_versions(self, rules, versions):
        """Get affected versions for maven, pypi, npm."""
        affected = []
        for ver in versions:
            for rule in rules:
                if len(rule) == 1:
                    if self._is_relation_applicable(rule[0]['key'], ver, rule[0]['val']):
                        affected.append(ver)
                elif len(rule) == 2:
                    key0 = rule[0]['key']
                    key1 = rule[1]['key']
                    first = self._is_relation_applicable(key0, ver, rule[0]['val'])
                    second = self._is_relation_applicable(key1, ver, rule[1]['val'])
                    if first and second:
                        affected.append(ver)
                    else:
                        if '=' in key0:
                            if self._is_relation_applicable("=", ver, rule[0]['val']):
                                affected.append(ver)
                        elif '=' in key1:
                            if self._is_relation_applicable("=", ver, rule[1]['val']):
                                affected.append(ver)
        return list(set(affected))

    def _get_semver_versions(self, versions):
        """Convert to semver version format."""
        semver = []
        for ver in versions:
            semver.append(api.to_semver(ver))
        return semver

    def _extract_data_from_feed(self):
        """Fetch all the required info from the feed."""
        for eco in self.snyk_data:
            if eco == "java" and self._parse_data_for_eco(eco):
                logger.info("Parsing feed for Maven.")
                self._parse_data(self.snyk_data[eco], "maven")
            elif eco == "js" and self._parse_data_for_eco(eco):
                logger.info("Parsing feed for Npm.")
                self._parse_data(self.snyk_data[eco], "npm")
            elif eco == "python" and self._parse_data_for_eco(eco):
                logger.info("Parsing feed for Pypi.")
                self._parse_data(self.snyk_data[eco], "pypi")
            else:
                logger.info("Ignoring the ecosystem {} from the feed".format(eco))

    def _parse_data(self, vuln_data, eco):
        """Parse data for all eco."""
        total_vuln = 0
        delta_mode = self.helper.is_delta_mode_on()
        if len(vuln_data) != 0:
            self.CVE_DATA[eco] = {}
            self.DELETE_CVE_DATA[eco] = []
            self.DELTA_FEED[eco] = []
            for data in vuln_data:
                if delta_mode and not data['modificationTime'].startswith(self.yesterday):
                    logger.debug("No new updates for {}".format(data['id']))
                    continue
                pkg = data['package']
                logger.debug("Fetching details for package: {}".format(pkg))
                try:
                    versions = None
                    if len(data['vulnerableVersions']) == 0:
                        logger.info("False positive found. {i}".format(i=data['id']))
                        self.DELETE_CVE_DATA[eco].append(data)
                        self.DELTA_FEED[eco].append(data)
                        continue
                    if pkg not in self.CVE_DATA[eco]:
                        resp_obj = get_versions_and_latest_for_ep(eco, pkg)
                        if resp_obj and (not isinstance(resp_obj, list)) \
                                and 'versions' in resp_obj and len(resp_obj['versions']) > 0:
                            versions = resp_obj['versions']
                            self.CVE_DATA[eco][pkg] = {
                                "affected": [],
                                "vulnerabilities": [],
                                "all_ver": versions,
                                "latest_version": resp_obj['latest_version'],
                                "ecosystem": eco,
                                "package": pkg
                            }
                    else:
                        versions = self.CVE_DATA[eco][pkg]['all_ver']
                    if versions:
                        logger.info("Processing {}".format(data['id']))
                        self.DELTA_FEED[eco].append(data)
                        data['ecosystem'] = eco
                        if eco in ['maven', 'pypi']:
                            vuln_versions = self._get_semver_versions(data['vulnerableVersions'])
                        else:
                            vuln_versions = data['vulnerableVersions']
                        data['rules'] = self._get_version_rules(vuln_versions)
                        data['affected'] = self._get_affected_versions(data['rules'], versions)
                        if len(data['affected']) == 0:
                            logger.info("No active affected version found for {}. Ignored."
                                        .format(pkg))
                            continue
                        total_vuln += 1
                        del data['rules']
                        self.CVE_DATA[eco][pkg]['affected'].extend(data['affected'])
                        self.CVE_DATA[eco][pkg]['affected'] = list(
                            set(self.CVE_DATA[eco][pkg]['affected']))
                        data['description'] = re.sub("[\'\"]", "", data['description'])
                        premium = str(data['premium']).lower() == 'true'
                        data['pvtVuln'] = premium
                        self.CVE_DATA[eco][pkg]['vulnerabilities'].append(data)
                except ValueError:
                    logger.error("Encountered an error while trying to fetch versions for "
                                 "{e} -> {p}".format(e=eco, p=pkg))
        logger.info("{} Data".format(eco).center(50, '-'))
        logger.info("Total affected packages: {}".format(len(self.CVE_DATA[eco])))
        logger.info("Total vulnerabilities: {}".format(total_vuln))
        logger.debug(self.CVE_DATA[eco])

    def _insert_cves(self):
        """Insert the cve data for each ecosystem."""
        logger.info("Insertion of data begins".center(50, '-'))
        dry_run = self.helper.is_dry_run()
        if "maven" in self.CVE_DATA:
            logger.info("Inserting Maven CVEs...")
            for pkg in self.CVE_DATA['maven']:
                cves = self.CVE_DATA['maven'][pkg]
                if len(cves['vulnerabilities']) > 0:
                    logger.info("Inserting CVEs for pkg: {}".format(pkg))
                    logger.debug(cves)
                    if not dry_run:
                        self.helper.make_api_call(cves, 'PUT')
                    logger.info("Waiting for 1 second".center(30, '-'))
                    time.sleep(1)

        if "npm" in self.CVE_DATA:
            logger.info("Inserting NPM CVEs...")
            for pkg in self.CVE_DATA['npm']:
                cves = self.CVE_DATA['npm'][pkg]
                if len(cves['vulnerabilities']) > 0:
                    logger.info("Inserting CVEs for pkg: {}".format(pkg))
                    logger.debug(cves)
                    if not dry_run:
                        self.helper.make_api_call(cves, 'PUT')
                    logger.info("Waiting for 1 second".center(30, '-'))
                    time.sleep(1)

        if "pypi" in self.CVE_DATA:
            logger.info("Inserting PyPi CVEs...")
            for pkg in self.CVE_DATA['pypi']:
                cves = self.CVE_DATA['pypi'][pkg]
                if len(cves['vulnerabilities']) > 0:
                    logger.info("Inserting CVEs for pkg: {}".format(pkg))
                    logger.debug(cves)
                    if not dry_run:
                        self.helper.make_api_call(cves, 'PUT')
                    logger.info("Waiting for 1 second".center(30, '-'))
                    time.sleep(1)
        return True

    def _delete_cves(self):
        """Delete the cve data for each ecosystem."""
        logger.info("Deletion of data begins".center(50, '-'))
        dry_run = self.helper.is_dry_run()
        del_obj = {'id': ''}
        if "maven" in self.DELETE_CVE_DATA:
            logger.info("Deleting false positive maven CVEs...")
            for vuln in self.DELETE_CVE_DATA['maven']:
                logger.info("Deleting {}".format(vuln['id']))
                del_obj['id'] = vuln['id']
                if not dry_run:
                    self.helper.make_api_call(del_obj, 'DELETE')
                logger.info("Waiting for 1 second".center(30, '-'))
                time.sleep(1)

        if "npm" in self.DELETE_CVE_DATA:
            logger.info("Deleting false positive npm CVEs...")
            for vuln in self.DELETE_CVE_DATA['npm']:
                logger.info("Deleting {}".format(vuln['id']))
                del_obj['id'] = vuln['id']
                if not dry_run:
                    self.helper.make_api_call(del_obj, 'DELETE')
                logger.info("Waiting for 1 second".center(30, '-'))
                time.sleep(1)

        if "pypi" in self.DELETE_CVE_DATA:
            logger.info("Deleting false positive pypi CVEs...")
            for vuln in self.DELETE_CVE_DATA['pypi']:
                logger.info("Deleting {}".format(vuln['id']))
                del_obj['id'] = vuln['id']
                if not dry_run:
                    self.helper.make_api_call(del_obj, 'DELETE')
                logger.info("Waiting for 1 second".center(30, '-'))
                time.sleep(1)
        logger.info("Deletion of data ends".center(50, '-'))
        return True

    def disable_snyk_run(self, date):
        """Enable or disable snyk run."""
        force = self.helper.force_run_ingestion()
        if force:
            return False
        return date.strftime('%H') != '06'

    def run_snyk_sync(self):
        """Entrypoint for the snyk cve sync process."""
        logger.info("Running Snyk Sync".center(50, '-'))
        logger.info(self.utc_now)
        if self.disable_snyk_run(self.utc_now):
            logger.info("Snyk sync to run only once a day. Won't run at this hour.")
            return

        if self.snyk_data:
            self._extract_data_from_feed()
            self._delete_cves()
            self._insert_cves()
            self.helper.store_json_content(self.DELTA_FEED, "snyk-feed/delta-feed-data/"
                                           + self.utc_now.strftime('%d-%m-%y') + ".json")
        else:
            logger.info("No data found. Snyk sync aborted.".center(50, '-'))
            return
        logger.info("Snyk Sync Process Successfully Completed".center(50, '-'))
        return "Success"
